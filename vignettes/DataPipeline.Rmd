---
title: "Data Pipeline"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(gsm)
library(gt)
library(dplyr)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

The `{gsm}` package provides a standardized workflow that leverages Key Risk Indicators (KRIs) and thresholds to conduct study-level Risk Based Quality Management (RBQM) for clinical trials. This vignette provides an overview of the `{gsm}` data model. First, we provide a summary of the standardized data pipeline used to calculate KRIs and evaluate thresholds to set site-level flags. We also discuss workflows and reports that allow users to run multiple KRIs concurrently on a study and/or create custom data workflows.

As an overview, clinical and operational data are analyzed using the `{gsm}` R package, which generate standardized outputs and reports suitable for use in various RBQM activities. The Appendices of this document provide detailed data input and output specifications for `{gsm}`.

# KRI Data Pipeline

In the context of clinical research, a KRI is a measure of risk associated with the conduct of a clinical trial. Examples of KRIs include the rate of adverse events or amount of missing data at a site or across sites. Defining and deploying KRIs during study start-up allows sponsors to continually monitor risks to the integrity of the trial and take corrective actions accordingly.

The default KRI data pipeline begins with one or more clinical datasets related to the KRI. An optional participant-level subset is then applied to each dataset before aggregating by participant to quantify the KRI at the participant level. This aggregated dataset is then further summarized by site to obtain site-level metrics.

## Generating Input Data

Generating input data is outside the scope of the assessment pipeline. The specifications for input data (i.e., `dfInput`) are designed to be flexible enough so that this data frame can be generated from multiple clinical data standards (e.g., Raw, rawplus, or CDISC-compliant ADaM). In order to generate input data, the user uses the dataframes outputted by the `mappings.yaml` file along with the `Input_Rate()` function to generate the `dfInput` needed for a particular 

For example, the Adverse Event assessment has two `Map()` functions:

TODO: add example


This `mapping.yaml` file is provided for convenience, but may not work for all clinical studies. When no relevant data.frame is produced with the `mapping.yaml`, the user can use the `RunQuery()` function to write a SQL query that produces the properly mapped and filtered dataframe. Details on this can be found in the [Cookbook Vignette](https://github.com/Gilead-BioStats/gsm/wiki/Cookbook-Vignette).

## Assessment Data Pipeline

![](data-analysis.PNG){width="170%"}

The image above provides an overview of the default KRI assessment pipeline. The pipeline is a standardized four-step process for assessing data issues by going from participant-level input data to a standardized site-level summary of model results. The functions used in each step of the data pipeline along with the input and output datasets are described in more detail below.

1.  `dfInput`: Input data; Cross-domain participant-level input data with all needed data for KRI derivation. Created by `Map()` functions as needed.
2.  `dfTransformed`: Transformed data; Site-level transformed data including KRI calculation. Created by `Transform()` functions.
3.  `dfAnalyzed`: Analyzed data; Site-level analysis results data. Created by `Analyze()` functions.
4.  `dfFlagged`: Flagged data; Site-level analysis results with flags added to indicate potential statistical outliers. Created by passing numeric thresholds to a `Flag()` function.
5.  `dfSummary`: Summary data; Standardized subset of the flagged data. This summary data has the same structure for all assessments and always includes both KRI and Flag values so that a user can easily look at trends for any given site across multiple assessments. Created using the `Summarize()` function.

Each KRI has a `.yaml` file that sequentially executes all of the supporting functions and returns a list of datasets and visualizations per the specifications of the .yaml file. The 4 standard datasets that are always returned are `dfTransformed`, `dfAnalyzed`, `dfFlagged`, and `dfSummary`. An additional dataset, called `dfBounds`, is generated only when a supported statistical model is being used. A step-by-step example of the data assessment workflow can be found in the [Data Analysis Vignette](https://gilead-biostats.github.io/gsm/articles/DataAnalysis.html).

Note that each step in this data pipeline can be customized based on the requirements for a specific KRI. The graphic below shows four such workflows.

![](data-analysis-combined.PNG){width="170%"}

# Custom Pipelines

## QTL and Country Analysis

The default KRI data pipeline runs site-level analyses on sourced clinical data. `{gsm}` also has the capability to run overall study-level analyses using Quality Tolerance Limits, or QTLs. For example, a user can evaluate the protocol deviation rate at a study-level. Analyses can also be categorized by country identifiers if desired. A user can also run custom pipelines (e.g. stratified analysis) as well (see [Appendix 3](#Appendix-3-Workflow-ID-Specs) for workflow nomenclature and examples).

## Running Multiple Assessments

Running multiple assessments simultaneously for a single study is a common use case of `{gsm}` and the package provides workflow and reporting functions to streamline this process.

TODO: update this to discuss use of yaml and MakeWorkflowList, RunWorkflow and RunStep

`RunWorkflow()` attempts to run one or more assessment workflows using shared data and metadata. The metadata used for this study-level assessment is described in detail in [Appendix 1](#Appendix-1-Metadata-Technical-Specifications). The `RunWorkflow()` function returns a list of assessments containing status information and results that is used as input for the reporting functions described below.

talk about `VisualizeMetric()` here

`Report_KRI()` creates a detailed report showing both charts and listings summarizing each KRI that was run for the study, along with a study-level table (via `Study_Table()`) and a summary of the workflow run to generate each KRI (via `Study_AssessmentReport()`) .

To see a sample report, run:

```{r, eval = FALSE}
# Prepare Workflows
wf_mapping <- MakeWorkflowList(strNames="mapping")[[1]]
wf_metrics <- MakeWorkflowList(strNames=paste0("kri",sprintf("%04d", 1:2)))
dfMetrics <- wf_metrics %>% map_df(~.x$meta)

# Import Site+Study Metadata
dfStudy<-clindata::ctms_study %>% rename(StudyID = protocol_number)
dfSite<- clindata::ctms_site %>% rename(SiteID = site_num)

# Pull Raw Data - this will overwrite the previous data pull
lRaw <- gsm::UseClindata(
  list(
    "dfSUBJ" = "clindata::rawplus_dm",
    "dfAE" = "clindata::rawplus_ae",
    "dfPD" = "clindata::ctms_protdev",
    "dfLB" = "clindata::rawplus_lb",
    "dfSTUDCOMP" = "clindata::rawplus_studcomp",
    "dfSDRGCOMP" = "clindata::rawplus_sdrgcomp",
    "dfDATACHG" = "clindata::edc_data_points",
    "dfDATAENT" = "clindata::edc_data_pages",
    "dfQUERY" = "clindata::edc_queries",
    "dfENROLL" = "clindata::rawplus_enroll"
  )
)

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, lData = lRaw)$lData

# Run Metrics
lResults <-wf_metrics %>% map(~RunWorkflow(lWorkflow=.x, lData=lMapped))

dfBounds <- lResults %>% 
  imap_dfr(~.x$lData$dfBounds %>% mutate(MetricID = .y)) %>%
  mutate(StudyID = "ABC-123") %>%
  mutate(SnapshotDate = Sys.Date())

dfSummary <- lResults %>% 
  imap_dfr(~.x$lData$dfSummary %>% mutate(MetricID = .y)) %>%
  mutate(StudyID = "ABC-123") %>%
  mutate(SnapshotDate = Sys.Date())

lCharts <- unique(dfSummary$MetricID) %>% map(~Visualize_Metric(
  dfSummary = dfSummary, 
  dfBounds = dfBounds, 
  dfSite = dfSite,
  dfMetrics = dfMetrics, 
  strMetricID = .x
)
) %>% setNames(unique(dfSummary$MetricID)) 

# Run reports 
strOutpath <- "gsm_site_report.html"
Report_KRI( lCharts = lCharts, 
            dfSummary = dfSummary,  
            dfSite = dfSite, 
            dfStudy = dfStudy, 
            dfMetrics = dfMetrics,
            strOutpath = strOutpath )
```

Additional examples are provided in the [Cookbook Vignette](https://gilead-biostats.github.io/gsm/articles/Cookbook.html).

# Appendix 1 - Data Pipeline Specifications

The following section provides a detailed specifications for the required columns for the data pipeline used in the `{gsm}` package.The following tables are used in the data pipeline:

-   `dfInput`: Input data; Cross-domain participant-level input data with all needed data for KRI derivation.
-   `dfTransformed`: Transformed data; Group-level transformed data including KRI calculation.
-   `dfAnalyzed`: Analyzed data; Group-level analysis results data with `Score` column added providing a statistical score for the group.
-   `dfFlagged`: Flagged data; Group-level analysis results with `Flag` column added to indicate potential statistical outliers.

Many columns are shared across tables

| Table    | Column Name  | Description                          | Type     |
|----------|--------------|--------------------------------------|----------|
| All      | MetricID     | The Metric ID                        | Character|
| All      | StudyID      | The Study ID                         | Character|
| All      | GroupID      | The group ID for the metric          | Character|
| All      | SnapshotDate | The Date of the snapshot             | Date     |
| All      | Numerator    | The calculated numerator value       | Numeric  |
| All      | Denominator  | The calculated denominator value     | Numeric  |
| All      | Metric       | The calculated rate/metric value     | Numeric  |
| dfInput  | SubjectID    | The subject ID                       | Character|
| dfAnalyzed/dfFlagged | Score        | The Stastical Score      | Numeric  |
| dfFlagged | Flag         | The Flag value                      | Numeric  |


## Appendix 2 - Workflow Specifications

Assessment workflow metadata objects are passed to the `lWorkflow` parameter in `RunWorkflow()` to define functions and parameters (including `mappings` and `specs`) across multiple studies.

The `lAssessment` object is a named list of metadata defining how each assessment should be run. By default, `MakeWorkflowList()` imports YAML specifications from `inst/workflow`. Each item in `lAssessments` expects the following parameters:

-   `workflow`: Array defining one or more functions to be executed as part of the workflow for a given assessment
    -   `workflow[]$name`: name of the `{gsm}` function.
    -   `workflow[]$inputs`: specifies the required input data
    -   `workflow[]$output`: specifies the output data from the workflow step, which can be used as an input in the next step in the workflow
    -   `workflow[]$params`: specifies parameters to be passed to the function

For example, the default workflow for the AE assessment (`inst/workflow/kri0001.yaml`) is shown below:

```{yaml, eval = FALSE}
steps:
  - name: FilterDomain
    inputs: dfSUBJ
    output: dfSUBJ
    params:
      strDomain: dfSUBJ
      strColParam: strEnrollCol
      strValParam: strEnrollVal
  - name: AE_Map_Raw
    inputs:
      - dfAE
      - dfSUBJ
    output: dfInput
  - name: AE_Assess
    inputs: dfInput
    output: lResults
    params:
      strGroup: "Site"
      vThreshold: null
      strMethod: "NormalApprox"
      nMinDenominator: 30
```
