---
title: "Data Pipeline"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(gsm)
library(gt)
library(dplyr)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

The `{gsm}` package provides a standardized workflow that leverages Key Risk Indicators (KRIs) and thresholds to conduct study-level Risk Based Quality Management (RBQM) for clinical trials. This vignette provides an overview of the `{gsm}` data model. First, we provide a summary of the standardized data pipeline used to calculate KRIs and evaluate thresholds to set site-level flags. We also discuss workflows and reports that allow users to run multiple KRIs concurrently on a study and/or create custom data workflows.

As an overview, clinical and operational data are analyzed using the `{gsm}` R package, which generate standardized outputs and reports suitable for use in various RBQM activities. The Appendices of this document provide detailed data input and output specifications for `{gsm}`. Below is a figure that depicts the full data pipeline, including both the Analysis and Reporting sections of the process.

![](data-model.PNG){width="100%"}

# KRI Data Pipeline

In the context of clinical research, a KRI is a measure of risk associated with the conduct of a clinical trial. Examples of KRIs include the rate of adverse events or amount of missing data at a site or across sites. Defining and deploying KRIs during study start-up allows sponsors to continually monitor risks to the integrity of the trial and take corrective actions accordingly.

The default KRI data pipeline begins with one or more clinical datasets related to the KRI. An optional participant-level subset is then applied to each dataset before aggregating by participant to quantify the KRI at the participant level. This aggregated dataset is then further summarized by site to obtain site-level metrics.

## Generating Input Data

Generating input data is outside the scope of the assessment pipeline. The specifications for input data (i.e., `dfInput`) are designed to be flexible enough so that this data frame can be generated from multiple clinical data standards (e.g., Raw, rawplus, or CDISC-compliant ADaM). In order to generate input data, the user uses the data frames outputted by the `data_mappings.yaml` file along with the `Input_Rate()` function to generate the `dfInput` needed for a particular KRI.


This `data_mapping.yaml` file is provided for convenience, but may not work for all clinical studies. When no relevant data.frame is produced with the `data_mapping.yaml`, the user can use the `RunQuery()` function to write a SQL query that produces the properly mapped and filtered dataframe. Details on this can be found in the [Cookbook Vignette](https://github.com/Gilead-BioStats/gsm/wiki/Cookbook-Vignette).

## Analysis Data Pipeline

![](data-analysis.PNG){width="100%"}

The image above provides an overview of the default KRI analysis pipeline. The pipeline is a standardized five-step process for assessing data issues by going from participant-level input data to a standardized site-level summary of model results. The functions used in each step of the data pipeline along with the input and output datasets are described in more detail below.

1.  `dfInput`: Input data; Cross-domain participant-level input data with all needed data for KRI derivation. Created by the `Input_Rate()` function as needed.
2.  `dfTransformed`: Transformed data; Site-level transformed data including KRI calculation. Created by `Transform_*()` functions.
3.  `dfAnalyzed`: Analyzed data; Site-level analysis results data. Created by `Analyze_*()` functions.
4.  `dfFlagged`: Flagged data; Site-level analysis results with flags added to indicate potential statistical outliers. Created by passing numeric thresholds to a `Flag_*()` function.
5.  `dfSummary`: Summary data; Standardized subset of the flagged data. This summary data has the same structure for all assessments and always includes both KRI and Flag values so that a user can easily look at trends for any given site across multiple assessments. Created using the `Summarize()` function.

Each KRI has a `.yaml` file that sequentially executes all of the supporting functions and returns a list of datasets and visualizations per the specifications of the .yaml file. In the default `yaml` files included in the `inst/workflow/metrics` directory of this package, the 5 standard datasets that are always returned are `dfTransformed`, `dfAnalyzed`, `dfFlagged`, `dfBounds`, and `dfSummary`. A step-by-step example of the data assessment workflow can be found in the [Data Analysis Vignette](https://gilead-biostats.github.io/gsm/articles/DataAnalysis.html).

Note that each step in this data pipeline can be customized based on the requirements for a specific KRI. The graphic below shows four such workflows.

![](data-analysis-combined.PNG){width="100%"}

## Reporting Data Pipeline

The pipeline discussed here is a standardized process for creating datasets by going from a standardized site-level summary of model results and external data sources to a series of datasets that provide all group-level data and metadata needed to create charts and reports for a given KRI. Unlike the analysis workflow, there is a single workflow for reporting, as all metric-specific information was captured in the analysis phase of the overall pipeline. The functions used in each step of the data pipeline along with the input and output datasets are described in more detail below.

1. `dfSummary`: Summary data; Standardized subset of the flagged data. This summary data has the same structure for all assessments and always includes both KRI and Flag values so that a user can easily look at trends for any given site across multiple assessments. Created using the `Summarize()` function.
4. `dfFlagged`: Flagged data; Site-level analysis results with flags added to indicate potential statistical outliers. Created by passing numeric thresholds to a `Flag_*()` function.
3. `dfBounds`: Bounded data; A data.frame containing predicted boundary values with upper and lower bounds across the range of observed values. Created with an `Analyze_*_PredictBounds()` function.
4. `dfGroups`: Grouped data; Long data.frame of summarized group CTMS data with site, study, and country level counts and metrics. Constructed by binding data.frames created with `MakeGroupInfo()`.
5. `dfMetrics`: Metric metadata; Metric-specific metadata for use in charts and reporting. Created by passing an `lWorkflow` object to the `MakeMetricInfo()` function.

The reporting data pipeline has a `yaml` file that runs through all steps to produce all five of these data.frames so that they can be used to create charts and reports. This `yaml` file can be found in `inst/workflows./data_reporting.yaml`. The `lData` list required to run this workflow with `RunWorkflow()` must contain the following elements:

1. `ctms_site`: CTMS site-level data set.
2. `ctms_study`: CTMS study-level data set.
3. `dfEnrolled`: data.frame of enrolled participants for this study.
4. `lWorkflows`: List of workflows, created with `MakeWorkflowList()`.  
5. `lAnalysis`: List of data.frames from the analysis workflow.
6. `dSnapshotDate`: Date of the snapshot (typically current date via `Sys.Date()`).
7. `strStudyID`: String of the study ID to be appended to data for use in charts and reports.

# Custom Pipelines

## QTL and Country Analysis

The default KRI data pipeline runs site-level analyses on sourced clinical data. `{gsm}` also has the capability to run overall study-level analyses using Quality Tolerance Limits, or QTLs. For example, a user can evaluate the protocol deviation rate at a study-level. Analyses can also be categorized by country identifiers if desired. A user can also run custom pipelines (e.g. stratified analysis) as well (see [Appendix 2](#Appendix-2-Workflow-ID-Specs) for workflow nomenclature and examples).

## Running Multiple KRIs

Running multiple assessments simultaneously for a single study is a common use case of `{gsm}` and the package provides workflow and reporting functions to streamline this process.

`RunWorkflows()` attempts to run one or more assessment workflows using shared data and metadata. The metadata used for this study-level assessment is described in detail in [Appendix 1](#Appendix-1-Metadata-Technical-Specifications). The `RunWorkflows()` function returns a list of assessments containing status information and results that is used as input for the reporting workflow and charting and reporting functions described below.

Once all reporting data.frames are created using the `data_reporting.yaml` workflow as discussed above, a user will create charts for all of the metrics specified in the analysis pipeline. `Visualize_Metric()` is a function that creates all available charts for a metric using the data provided. If the `dfSummary` provided to `Visualize_Metric()` contains multiple data with multiple dates, then the `timeSeries` charts will be included in the list of charts returned. The output of this function is a list containing the following charts:

 - `scatterJS`: A scatter plot using JavaScript.
 - `scatter`: A scatter plot using ggplot2.
 - `barMetricJS`: A bar chart using JavaScript with metric on the y-axis.
 - `barScoreJS`: A bar chart using JavaScript with score on the y-axis.
 - `barMetric`: A bar chart using ggplot2 with metric on the y-axis.
 - `barScore`: A bar chart using ggplot2 with score on the y-axis.
 - `timeSeriesContinuousMetricJS`: A time series chart using JavaScript with metric on the y-axis.
 - `timeSeriesContinuousScoreJS`: A time series chart using JavaScript with score on the y-axis.
 - `timeSeriesContinuousNumeratorJS`: A time series chart using JavaScript with numerator on the y-axis.

Now that all imputs for the `Report_KRI()` function have been produced, it can be called to create a detailed report showing both charts and listings summarizing each KRI that was run for the study.

Putting all of these steps together, the following script will create a sample report using both the analysis and reporting workflows for the AE KRI.

```{r, eval = FALSE}
lRaw <- list(
    dfSUBJ = clindata::rawplus_dm,
    dfAE = clindata::rawplus_ae,
    dfPD = clindata::ctms_protdev,
    dfLB = clindata::rawplus_lb,
    dfSTUDCOMP = clindata::rawplus_studcomp,
    dfSDRGCOMP = clindata::rawplus_sdrgcomp %>% dplyr::filter(.data$phase == 'Blinded Study Drug Completion'),
    dfDATACHG = clindata::edc_data_points,
    dfDATAENT = clindata::edc_data_pages,
    dfQUERY = clindata::edc_queries,
    dfENROLL = clindata::rawplus_enroll
)

# Make Workflow Lists
wf_mapping <- MakeWorkflowList(strNames = "mapping")
wf_kri <- MakeWorkflowList(strNames="kri0001")
wf_reporting <- MakeWorkflowList(strNames = "reporting")

# Generate Mapped Data
lMapped <- RunWorkflows(lWorkflow = wf_mapping, lData = lRaw)

# Generate Analysis Results Data
lAnalysis <- RunWorkflows(lWorkflow = wf_kri, lData = lMapped)

# Generate Reporting Data
lReporting_Input <- list(
    ctms_site = clindata::ctms_site,
    ctms_study = clindata::ctms_study,
    dfEnrolled =lMapped$dfEnrolled,
    lWorkflows = wf_kri,
    lAnalysis = list(kri0001 = lAnalysis),
    dSnapshotDate = Sys.Date(),
    strStudyID = "ABC-123"
)

lReporting <- RunWorkflows(lWorkflow = wf_reporting, lData = lReporting_Input)

# Convenience Mappings
dfGroups <- lReporting$dfGroups
dfMetrics <- lReporting$dfMetrics
dfSummary <- lReporting$dfSummary
dfBounds <- lReporting$dfBounds

# Create dfSite and dfStudy for charts
dfSite <- dfGroups %>%
  filter(GroupLevel == "Site") %>%
  pivot_wider(names_from=Param, values_from=Value) %>%
  rename(
    SiteID = GroupID,
    status = Status,
    enrolled_participants = ParticipantCount
  )

dfStudy <-  dfGroups %>% 
  filter(GroupLevel == "Study") %>% 
  pivot_wider(names_from=Param, values_from=Value)

# Create Charts for all metrics
options(vsc.viewer = FALSE)
metrics <- unique(dfMetrics$MetricID)
charts <- metrics %>% map(~Visualize_Metric(
  dfSummary = dfSummary,
  dfBounds = dfBounds,
  dfGroups = dfGroups,
  dfMetrics = dfMetrics,
  strMetricID = .x
)) %>% setNames(metrics)

##function not currently working!
# Report_KRI(lCharts = charts,
#            dfSummary = dfSummary, 
#            dfSite = dfSite,
#            dfStudy = dfStudy,
#            dfMetrics = dfMetrics,
#            strOutput = "~/test_report_kri.html")
```

Additional examples are provided in the [Cookbook Vignette](https://gilead-biostats.github.io/gsm/articles/Cookbook.html).

# Appendix 1 - Data Model

# Overview 
![](data-model.PNG){width="100%"}

# Analytics data model

The KRI analytics pipeline is a standardized process for **Analyzing** data issues by going from participant-level `input` data to a standardized site-level `summary` of model results. The data sets used in each step of the data pipeline are described in detail below.

## Analytics Data Tables
  
### `dfInput`
  - Function(s) used to create table: 
    - `Input_Rate()`
  - Inputs: 
    - `dfSubjects`
    - `dfNumerator`
    - `dfDenominator`
  - Usage: The base data.frame for all Analytics workflows. Feeds into the `Transform_*()` functions.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | 
  |----------|--------------|--------------------------------------|----------|--|
  | dfInput  | SubjectID    | The subject ID                       | Character| |
  | dfInput  | GroupID      | The group ID for the metric          | Character| | |
  | dfInput  | GroupLevel   | The group type for the metric (e.g. "Site")    | Character| | 
  | dfInput  | Numerator    | The calculated numerator value       | Numeric  | |
  | dfInput  | Denominator  | The calculated denominator value     | Numeric | |
  | dfInput  | Metric       | The calculated rate/metric value     | Numeric  | |


### `dfTransformed`
  - Function(s) used to create table:
    - `Transform_Rate()`
    - `Transform_Count()`
  - Inputs: `dfInput`
  - Usage: Convert from input data format to needed format to derive KRI for an Assessment via the `Analyze_*()` functions.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | 
  |----------|--------------|--------------------------------------|----------|--|
  | dfTransformed | GroupID      | The group ID for the metric          | Character| | |
  | dfTransformed | GroupLevel   | The group type for the metric (e.g. "Site")    | Character| | 
  | dfTransformed | Numerator    | The calculated numerator value       | Numeric  | |
  | dfTransformed | Denominator  | The calculated denominator value     | Numeric | |
  | dfTransformed | Metric       | The calculated rate/metric value     | Numeric  | |

###  `dfAnalyzed`
  - Function(s) used to create table:
    - `Analyze_Fisher()`
    - `Analyze_Identity()`
    - `Analyze_NormalApprox()`
    - `Analyze_Poisson()`
    - `Analyze_QTL()`
  - Inputs: `dfTransformed`
  - Usage: Prepare the data for `Flag_*()` by performing the specified test on the metric provided.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | 
  |----------|--------------|--------------------------------------|----------|--|
  | dfAnalyzed | GroupID      | The group ID for the metric          | Character| | |
  | dfAnalyzed | GroupLevel   | The group type for the metric (e.g. "Site")    | Character| | 
  | dfAnalyzed | Numerator    | The calculated numerator value       | Numeric  | |
  | dfAnalyzed | Denominator  | The calculated denominator value     | Numeric | |
  | dfAnalyzed | Metric       | The calculated rate/metric value     | Numeric  | |
  | dfAnalyzed | Score        | The Statistical Score      | Numeric  | |
  | dfAnalyzed | Overall Metric |       | Numeric  |* |
  | dfAnalyzed | Factor |      | Numeric  |* |
  | dfAnalyzed | Predicted Count |      | Numeric  |* |


### `dfBounds`
  - Function(s) used to create table:
    - `Analyze_NormalApprox_PredictBounds()`
    - `Analyze_Poisson_PredictBounds()`
  - Inputs: `dfTransformed`
  - Usage: Calculates predicted percentages/rates and upper- and lower-bounds across the full range of sample sizes/total exposure values for reporting.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | 
  |----------|--------------|--------------------------------------|----------|--|
  | dfBounds | Threshold    | The number of standard deviations that the upper and lower bounds are based on   | Numeric| | |
  | dfBounds | Denominator  | The calculated denominator value     | Numeric | |
  | dfBounds | LogDenominator  | The calculated log denominator value     | Numeric | |
  | dfBounds | Numerator    | The calculated numerator value       | Numeric  | |
  | dfBounds | Metric       | The calculated rate/metric value     | Numeric  | |
  | dfBounds | MetricID     | The Metric ID                        | Character| * |
  | dfBounds | StudyID      | The Study ID                         | Character| * |
  | dfBounds | SnapshotDate | The Date of the snapshot             | Date     | * |
  
### `dfFlagged`
  - Function(s) used to create table:
    - `Flag_Fisher()`
    - `Flag_Identity()`
    - `Flag_NormalApprox()`
    - `Flag_Poisson()`
    - `Flag_QTL()`
  - Inputs: `dfAnalyzed`
  - Usage: Flag a group-level metric to be summarized via `Summarize()` and used for reporting.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | 
  |----------|--------------|--------------------------------------|----------|--|
  | dfFlagged | GroupID      | The group ID for the metric          | Character| | |
  | dfFlagged | GroupLevel   | The group type for the metric (e.g. "Site")    | Character| | 
  | dfFlagged | Numerator    | The calculated numerator value       | Numeric  | |
  | dfFlagged | Denominator  | The calculated denominator value     | Numeric | |
  | dfFlagged | Metric       | The calculated rate/metric value     | Numeric  | |
  | dfFlagged | Score        | The Statistical Score     | Numeric  | |  
  | dfFlagged | Flag        | The ordinal Flag to be applied     | Numeric  | |  
  | dfFlagged | Overall Metric |       | Numeric  |* |
  | dfFlagged | Factor |      | Numeric  |* |
  | dfFlagged | Predicted Count |      | Numeric  |* |

###  `dfSummary`
  - Function(s) used to create table:
    - `Summarize()`
  - Inputs: `dfFlagged`
  - Usage: Summarize KRI at the group level for reporting.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | 
  |----------|--------------|--------------------------------------|----------|--|
  | dfSummary      | GroupID      | The group ID for the metric          | Character| | |
  | dfSummary      | GroupLevel   | The group type for the metric (e.g. "Site")    | Character| | 
  | dfSummary      | Numerator    | The calculated numerator value       | Numeric  | |
  | dfSummary      | Denominator  | The calculated denominator value     | Numeric | |
  | dfSummary      | Metric       | The calculated rate/metric value     | Numeric  | |
  | dfSummary      | MetricID     | The Metric ID                        | Character| * |
  | dfSummary      | StudyID      | The Study ID                         | Character| * |
  | dfSummary      | SnapshotDate | The Date of the snapshot             | Date     | * |

# Overview of Reporting data model
  
## Reporting Data Tables
  
###  `dfSummary`
  - Function(s) used to create table:
    - `Summarize()`
  - Inputs: `dfFlagged`
  - Usage: Summarize KRI at the group level for reporting.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  |  rbm-viz Column Name |
  |----------|--------------|--------------------------------------|----------|--|-----------|
  | dfSummary      | GroupID      | The group ID for the metric          | Character| | groupid |
  | dfSummary      | GroupLevel   | The group type for the metric (e.g. "Site")    | Character| | - |
  | dfSummary      | Numerator    | The calculated numerator value       | Numeric  | | numerator |
  | dfSummary      | Denominator  | The calculated denominator value     | Numeric | | denominator |
  | dfSummary      | Metric       | The calculated rate/metric value     | Numeric  | | metric |
  | dfSummary      | Score        | The calculated metric score          | Numeric  | | score |
  | dfSummary      | Flag         | The calculated flag                  | Numeric  | | flag |
  | dfSummary      | MetricID     | The Metric ID                        | Character| * | workflowid |
  | dfSummary      | StudyID      | The Study ID                         | Character| * | studyid |
  | dfSummary      | SnapshotDate | The Date of the snapshot             | Date     | * | snapshot_date |

### `dfBounds`
  - Function(s) used to create table:
    - `Analyze_NormalApprox_PredictBounds()`
    - `Analyze_Poisson_PredictBounds()`
  - Inputs: `dfTransformed`
  - Usage: Calculates predicted percentages/rates and upper- and lower-bounds across the full range of sample sizes/total exposure values for reporting.
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  |   rbm-viz Column Name |
  |----------|--------------|--------------------------------------|----------|--|----------|
  | dfBounds | Threshold    | The number of standard deviations that the upper and lower bounds are based on   | Numeric| | threshold |
  | dfBounds | Denominator  | The calculated denominator value     | Numeric | | denominator |
  | dfBounds | LogDenominator  | The calculated log denominator value     | Numeric | | log_denominator |
  | dfBounds | Numerator    | The calculated numerator value       | Numeric  | | numerator |
  | dfBounds | Metric       | The calculated rate/metric value     | Numeric  | | - |
  | dfBounds | MetricID     | The Metric ID                        | Character| * | workflowid |
  | dfBounds | StudyID      | The Study ID                         | Character| * | studyid |
  | dfBounds | SnapshotDate | The Date of the snapshot             | Date     | * | snapshot_date |

###  `dfGroups`
  - Workflow used to create table: `GroupMeta`
  - Inputs: CTMS site, study and country data
  - Usage: Group-level metadata dictionary.
  - Structure: Long data frame, with certain `Param` required for given `GroupLevel`


| Table       | Column                | Description                       |Type      | Optional  |  
|-------------|-----------------------|-----------------------------------|----------|-----------|
| dfGroups | ProtocolID               | Protocol ID                       | Character | |
| dfGroups | SnapshotDate             | Snapshot Date                     | Character | |
| dfGroups | GroupID                  | Unique Group ID                   | Character| | 
| dfGroups | GroupLevel               | Group Level (e.g. Site, Country)  | Character| |
| dfGroups | Param                    | Parameter Name (e.g. "Status")  | Character| | 
| dfGroups | Value                    | Parameter Value (e.g. "Active")        | Character| | 

Expected `Param` by `GroupLevel` for use in gsm reporting. Fine to add other Param values as needed. 

| GroupLevel  | Param                | Description                       |Value Type      |
|-------------|----------------------|-----------------------------------|----------|
| Study       | Status               | Study Status                      | Character| 
| Study       | Title       | Protocol Title       | Numeric | 
| Study       | ParticipantCount       | # of Enrolled Participants        | Numeric | 
| Study       | SiteCount              | # of Activated Sites                 | Numeric| 
| Study       | ParticipantsPlanned        | # of Planned Participants        | Numeric| 
| Study       | SitesPlanned               | # of Planned Sites        | Numeric| 
| Site       | ParticipantCount       | # of Enrolled Participants        | Numeric | 
| Site       | Status       | Site Status       | Character | 
| Site       | InvestigatorFirstName       | Investigator First name        | Character | 
| Site       | InvestigatorLastName       | Investigator Last name        | Character | 
| Site       | City        | City       | Character| 
| Site       | State       | State      | Character | 
| Site       | Country       | Country       | Character | 
| Country       | EnrolledParticipants       | # of Enrolled Participants        | Numeric | 


###  `dfMetrics`
  - Function used to create table:
  - Inputs:
  - Usage:
  - Structure:

  | Table    | Column Name  | Description                          | Type     | Optional  | rbm-viz Column Name |
  |----------|--------------|--------------------------------------|----------|--| -------------- |
  | dfMetrics| File         | The yaml file for workflow          | Character| | file          | 
  | dfMetrics| MetricID     | ID for the Metric   | Character| | workflowid |
  | dfMetrics| Group        | The group type for the metric (e.g. "Site")      | Character| | group |
  | dfMetrics| Abbreviation | Abbreviation for the metric   | Character| | abbreviations |
  | dfMetrics| Metric       | Name of the metric    | Character| | metric |
  | dfMetrics| Numerator    | Data source for the Numerator          | Character| |numerator      |
  | dfMetrics| Denominator  | Data source for the Denominator | Character| | denominator   |
  | dfMetrics| Model        | Model used to calculate metric     | Character| | model         |
  | dfMetrics| Score        | Type of Score reported      | Character| | score         |
  | dfMetrics| strThreshold   | Thresholds to be used for bounds and flags     | Character| | vthreshold    |


# Previous Versions

###  `dfSite`
  - Function used to create table:
  - Inputs:
  - Usage:
  - Structure:

| Table       | Column                | Description                       |Type      | Optional  |   rbm-viz Column Name |
|-------------|-----------------------|-----------------------------------|----------|-----------|-----------|
| dfSite | protocol_row_id       | Protocol row ID                   | Character | |protocol_row_id  | 
| dfSite | SiteID                | Unique Site ID                    | Character| | site_id  |
| dfSite | site_row_id           | Site row ID                       | Character| | site_row_id  |
| dfSite | protocol              | Protocol ID                       | Character| | protocol  |
| dfSite | pi_number             | Principal Investigator Number     | Character| | pi_number |
| dfSite | pi_last_name          | Principal Investigator Last Name  | Character| | pi_last_name|
| dfSite | pi_first_name         | Principal Investigator First Name | Character| | pi_first_name |
| dfSite | site_status           | Site Status                       | Character | | site_status |
| dfSite | is_satellite          | Is site a satellite location      | Character| | is_satellite  |
| dfSite | account               |                                   | Character| | account   |
| dfSite | site_active_dt        | Date that Site became active      | Character| | site_active_dt |
| dfSite | city                  | Site City                         | Character| | city |
| dfSite | state                 | Site State                        | Character| | state  |
| dfSite | country               | Site Country                      | Character| | country |

###  `dfStudy`
  - Function used to create table:
  - Inputs:
  - Usage:
  - Structure:

| Table       | Column                | Description                       |Type      | Optional  |
|-------------|-----------------------|-----------------------------------|----------|-----------|
| dfStudy| protocol_row_id       | Protocol row ID                   | Character | |
| dfStudy| StudyID               | Unique Study ID                   | Character| |
| dfStudy| protocol_title        | Protocol Title                    | Character| |
| dfStudy| nickname              | Protocol Nickname  | Character| |
| dfStudy| protocol_type         | Protocol Type                     | Character| |
| dfStudy| phase                 | Study phase  | Character| |
| dfStudy| num_plan_site         | Number of planned sites in the study | Character| |
| dfStudy| num_site_actl         | Number of active sites in the study | Character | |
| dfStudy| est_fpfv              | Estimated first patient first visit      | Date   | |
| dfStudy| act_fpfv              | Actual first patient first visit      | Date   | |
| dfStudy| est_lplv              | Estimated last patient last visit     | Date   | |
| dfStudy| act_lplv              | Actual last patient last visit       | Date   | |
| dfStudy| est_lpfv              | Estimated last patient first visit     | Date   | |
| dfStudy| act_lpfv              | Actual last patient first visit       | Date   | |
| dfStudy| status                | Study Status                       | Character| |
| dfStudy| num_plan_subj         | Number of planned subjects in study      | Numeric | |
| dfStudy| num_enrolled_subj_m   | Number of enrolled subjects in study | Numeric | |
| dfStudy| protocol_indication   | Protocol Indication                        | Character| |
| dfStudy| product               | Product                      | Character| |
| dfStudy| therapeutic_area      | Therapeutic Area                 | Character| |
| dfStudy| protocol_product_number| Protocol Product Number                      | Numeric | |
| dfStudy| x_rbm_flg             | Is RBM Flagged                      | Character| |



## Appendix 2 - Analysis Workflow Specifications

Assessment workflow metadata objects are passed to the `lWorkflow` parameter in `RunWorkflow()` to define functions and parameters across multiple studies.

The `lWorkflow` object is a named list of metadata and steps defining how each assessment should be run. By default, `MakeWorkflowList()` imports YAML specifications from `inst/workflow/metrics`. Each item in `lWorkflow` expects the following parameters in the `steps` section:

-   `workflow`: Array defining one or more functions to be executed as part of the workflow for a given assessment
    -   `workflow[]$meta`: specifies all of the metadata information for the KRI.
    -   `workflow[]$steps`: specifies all of the steps in the workflow.
        -   `workflow[]$steps$name`: name of the `{gsm}` function.
        -   `workflow[]$steps$inputs`: specifies the required input data
        -   `workflow[]$steps$output`: specifies the output data from the workflow step, which can be used as an input in the next step in the workflow
        -   `workflow[]$steps$params`: specifies parameters to be passed to the function

For example, the default workflow for the AE assessment (`inst/workflow/metrics/kri0001.yaml`) is shown below:

```{yaml, eval = FALSE}
meta:
  MetricID: kri0001
  File: kri0001.yaml
  Group: site
  Abbreviation: AE
  Metric: Adverse Event Rate
  Numerator: Adverse Events
  Denominator: Days on Study
  Model: Normal Approximation
  Score: Adjusted Z-Score
  vThreshold: 
    - -2
    - -1
    - 2
    - 3
  nMinDenominator: 30
steps:
  - name: Input_Rate
    output: dfInput
    params:
      dfSubjects: dfEnrolled
      dfNumerator: dfAE
      dfDenominator: dfEnrolled
      strSubjectCol: subjid
      strGroupCol: siteid
      strNumeratorMethod: Count
      strDenominatorMethod: Sum
      strDenominatorCol: timeonstudy
  - name: Transform_Rate
    output: dfTransformed
    params:
      dfInput: dfInput
  - name: Analyze_NormalApprox
    output: dfAnalyzed
    params:
      dfTransformed: dfTransformed
      strType: rate
  - name: Analyze_NormalApprox_PredictBounds
    output: dfBounds
    params:
      dfTransformed: dfTransformed
      vThreshold: vThreshold
      strType: rate
  - name: Flag_NormalApprox
    output: dfFlagged
    params:
      dfAnalyzed: dfAnalyzed
      vThreshold: vThreshold
  - name: Summarize
    output: dfSummary
    params: 
      dfFlagged: dfFlagged
      nMinDenominator: nMinDenominator
```
