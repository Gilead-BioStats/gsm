---
title: "Qualification Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Qualification Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(gsm)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Intro

Qualification for this repository is done to ensure that the package is functioning as intended.  While unit tests should also be written to test the code, qualification testing is used to test that the expected behaviors are happening correctly.  Qualification is done using a set of machine-readable documents and associated functions to create a strong documentation structure as well as a cohesive qualification report.  This qualification process will be modified to add new assessments and should be updated whenever there are updates that affect the workflows tested by qualification tests.  Qualification tests are designed to provide developers with a repeatable process that is easy to update and document.

# Process Overview

Each GSM assessment is independently qualified using **Specifications** and **Test Cases**, which are then compiled into a **Qualification Report**. 

- **Specifications** - the expected behaviors that are being tested. 
- **Test cases** - testable pieces of code associated with Specifications.
- **Qualification Report** - Summary snapshot of all qualification activity.

# Specifications 

Specifications should capture the most important use cases for an assessment. One set of specifications will be created per assessment. Each assessment is likely to have several specifications, and each specification should have one or more associated test cases. 

Each Specification should include the following components:  

- **Description** - Describes the use case for the specification.    
- **Risk Assessment** - Evaluation of risk for the the use case. Includes 2 components:  
  - **Risk Severity** - Risk Severity can be low, medium, or high and corresponds to the risk level that this specification would fail.  
  - **Risk Impact** - Risk Impact can be low, medium, or high and corresponds to the impact that the specification failing would have.  
- **Test Cases** - A list of test cases associated with the specification.  

The specifications (including Description, Risk Severity, and Risk Impact) should be documented in `qualification_specs.csv`, to be rendered by the Qualification Report, documented in a later section. 
For example, the first specification from `qualification_specs.csv` is written as: 

```
- ID: S1_1
  Description: Given correct input data an Adverse Event assessment can be done using the Poisson method
  Risk: High
  Impact: High
  Tests:
    - T1_1
    - T1_2
``` 

# Test Cases

Test Cases take specifications and turn them into the actual testable pieces that make up the larger specification.  The Test Cases are built to confirm that the package meets the requirements laid out for it.  Test Cases should be representative of how a user may utilize the program since that will be the most helpful to uncovering defects and supporting test automation.  Test Cases are linked to the assessment as shown above.

Test Cases are written using the standard `testthat` workflow and saved in `tests/testqualification/qualification`.  Each test case should be saved as it's own file and named using with the convention `test_qual_{TestID}.R` where TestID corresponds to the `Tests` section in the relevant specification.

Note that Test Cases can be associated with multiple Specifications.  For example, S1_1 includes tests (T1_1, T1_2, T1_3) that the AE_Assess function is performed properly for the Poisson method.  Each of these tests (T1_1, T1_2, T1_3) tests whether the Poisson method output is accurate when the data is grouped by a different grouping variable (Site, Study, Custom, respectively).  In addition, the input data for T1_1 and T1_2 are a subset of a larger dataframe, and thus T1_1 and T1_2 also test whether the AE_Assess function performs appropriately when provided subsetted input data, which satisfies spec S1_6.  

Test code should be written in a clear and concise manner making it quick to execute and easy to review and interpret.  Test cases should also have an informative description to outline what is being tested.

An simple example test case is shown below.  For this test case the file would be called `test_qual_T1_1.R` and would correspond to T1_1 in the specifications:

```r
test_that("Domain-level data can be correctly merged into subject-level data using subject ID as the key variable.", {


  ########### gsm mapping ###########
  observed <- gsm::MergeSubjects(
    dfDomain = clindata::rawplus_consent,
    dfSUBJ = clindata::rawplus_dm,
    strIDCol = "subjid"
    )


  ########### double programming ###########
  # read in raw consent data
  consent_orig <- clindata::rawplus_consent

  # read in raw source DM data
  dm_raw_orig <- clindata::rawplus_dm

  # join DM and consent data
  expected <- left_join(dm_raw_orig, consent_orig, by = "subjid")

  ########### testing ###########
  expect_equal(as.data.frame(observed), as.data.frame(expected))
})
```

# Qualification Report

The Qualification Report is generated to document and display the code and the qualification that code has been through.  The report lives as a Qualification vignette in `{gsm}` and rendered during other workflows.  The Qualification Report is also attached to each release and included in the `{pkgdown}` site to display the qualification status of `{gsm}`.  The sections of the Qualification Report are outlined below.

### Qualification Testing Environment

The `sessionInfo()` of the qualification environment is included to show what environment the tests were run in.  This is called after all necessary packages have been loaded and all setup is done. The environment should not change after this part of the report is created.

### Qualification Testing Results

Using the Specifications, Test Cases, and test code outlined above the qualification status of all assessments currently qualified within `{gsm}` is rendered, consisting of smaller sections for each assessment.  These smaller sections will include the procedure that is being qualified, which should correspond to the function that is used for that procedure.  An overview of the Specifications is also included that has the ID, Description, Risk, Impact, and associated Test Cases corresponding to each Specification.  This information is pulled from the Specification Spreadsheet file (`qualification_specs.csv`) outlined above.  A Traceability Matrix is also included to show what Specifications are covered by what Test Cases.  The last part is an overview of the Test Cases that correspond to the procedure or function.  A table is displayed to show the Test Case ID, Description, and the pass/fail status of the test case.  

### Unit Tests

A summary of the unit test coverage is included in the Qualification Report to show how well the package functions are unit tested.  This is created by using `covr::package_coverage()` and then listing it out by function.

### Pull Requests

The final section of the Qualification Report is an overview of all Pull Requests since last release.  This includes the title, compare and base branches, a link to the GitHub page, requester, reviewers, date requested, and the status of the Pull Request.  While this is meant to be a comprehensive overview of the Pull Requests the release documentation should also include links to all Pull Requests included in the release.
