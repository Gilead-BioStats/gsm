---
title: "Qualification Workflow"
date: 'Date: `r Sys.Date()`'
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Qualification Workflow}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---

```{r setup, include = FALSE}
suppressPackageStartupMessages({
  library(gsm)
  library(gsm.mapping)
  library(knitr)
  library(gt)
  library(dplyr)
  library(purrr)
  library(pander)
  library(gh)
  library(stringr)
  library(riskmetric)
  suppressMessages(devtools::load_all())
})

opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE,
  echo = FALSE,
  results = "asis",
  message = FALSE,
  tidy = FALSE
)

source(here::here("inst", "qualification", "specs.R"))
source(here::here("inst", "qualification", "test_cases.R"))
spec_df <- import_specs()
```

# Introduction

Qualification for this repository is done to ensure that the package is functioning as intended and that core functions execute as expected on a system-wide scale.  While unit tests should also be written to test the code, qualification testing is used to test that the expected behaviors are happening correctly.  Qualification is done using a set of machine-readable documents and associated functions to create a strong documentation structure as well as a cohesive qualification report.  This qualification process will be modified to add new assessments and should be updated whenever there are updates that affect the workflows tested by qualification tests.  Qualification tests are designed to provide developers with a repeatable process that is easy to update and document.

# Process Overview

Each GSM assessment is independently qualified using **Specifications** and **Test Cases**, which are then compiled into a **Qualification Report**. 

- **Specifications** - the expected behaviors that are being tested. 
- **Test Cases** - testable pieces of code associated with Specifications.
- **Qualification Report** - Summary snapshot of all qualification activity.

# Specifications 

Specifications should capture the most important use cases for a given function. Each function must have at least one
(1) specification, and each specification must have at least one (1) associated test case. Multiple specifications
may exist for a function, and multiple test cases may exist for a specification.

Each Specification should include the following components:  

- **Description** - Outlines the use case for the specification.    
- **Risk Assessment** - An evaluation of risk for the the use case. Includes 2 components:  
  - **Risk Level** - Risk Level can be "Low," "Medium," or "High," corresponding to the risk associated with the specification failing.  
  - **Risk Impact** - Risk Impact can be "Low," "Medium," or "High," corresponding to the severity of the impact associated with the specification failing.  
- **Test Cases** - A list of test cases associated with the specification.  

The specifications (including Description, Risk Level, and Risk Impact) should be documented in `qualification_specs.csv`, to be rendered by the Qualification Report, documented in a later section. 
For example, the first specification from `qualification_specs.csv` is written as: 


```{r}
qual_specs_orig <- read.csv(system.file("qualification", "qualification_specs.csv", package = "gsm"))
qual_specs <- qual_specs_orig %>%
  mutate(SpecID = paste0("S", Spec, "_", Test.ID)) %>%
  select(Specs = SpecID, Tests, N.Tests)

### One row per spec
Specifications <- spec_df %>%
  select(ID, Description, Risk, Impact, Tests) %>%
  mutate(sort1 = as.numeric(str_extract(ID, "(?<=S)\\d+")),
         sort2 = as.numeric(str_extract(ID, "\\d+$"))) %>%
  arrange(as.numeric(sort1), as.numeric(sort2)) %>%
  select(-c(sort1, sort2))
colnames(Specifications) <- c("Spec ID", "Spec Description", "Risk", "Impact", "Associated Test IDs")

Specifications %>%
  gt() %>%
  tab_options(
    table.font.size = px(9)
  )%>%
  cols_width(
    `Spec Description` ~ px(180), 
    `Associated Test IDs` ~ px(150) 
  ) %>%
  opt_row_striping() 
```  

# Test Cases

Test cases translate specifications into testable scripts to confirm that the package functions meet the
established requirements. Test cases should be representative of how a user may utilize the function to help identify code gaps and support testing automation. Test cases are linked to the assessment as shown above.

Test cases are written using the standard `testthat` workflow and saved in `tests/testqualification/qualification`.  Each test case should be saved as an individual file and named using the convention `test_qual_{TestID}.R`, where TestID corresponds to the test case number. Test code within these scripts should be written clearly and concisely to facilitate quick execution, review, and interpretation. Test cases should also have an informative description to outline what is being tested.

Note that test cases can be associated with multiple specifications.  For example, S1_1 includes tests (T1_1, T1_2, T1_3) that the AE_Assess function is performed properly for the Poisson method.  Each of these tests (T1_1, T1_2, T1_3) tests whether the Poisson method output is accurate when the data is grouped by a different grouping variable (Site, Study, Custom, respectively).  In addition, the input data for T1_1 and T1_2 are a subset of a larger dataframe, and thus T1_1 and T1_2 also test whether the AE_Assess function performs appropriately when provided a subset of the input data, which satisfies spec S1_6.  

An simple example test case is shown below.  For this test case the file would be called `test_qual_T21_1.R` and would correspond to T21_1 in the specifications:

```{r eval = F, include = T}
testthat::test_that("Given raw participant-level data, a properly specified Workflow for a KRI creates summarized and flagged data", {
  test <- robust_runworkflow(kri_workflows, mapped_data)
  expected_rows <- length(na.omit(unique(test$dfEnrolled[[kri_workflows$steps[[2]]$params$strGroupCol]])))

  # test output stucture
  expect_true(is.vector(test$vThreshold))
  expect_true(all(map_lgl(test[outputs[outputs != "vThreshold"]], is.data.frame)))
  expect_equal(nrow(test$dfFlagged), expected_rows)
  expect_equal(nrow(test$dfSummary), expected_rows)

  # test output content
  expect_true(all(outputs %in% names(test)))
  flags <- test$dfFlagged %>%
    mutate(hardcode_flag = case_when(
      Score <= test$vThreshold[1] ~ -2,
      Score > test$vThreshold[1] & Score <= test$vThreshold[2] ~ -1,
      Score >= test$vThreshold[3] & Score < test$vThreshold[4] ~ 1,
      Score >= test$vThreshold[4] ~ 2,
      TRUE ~ 0
    )) %>%
    left_join(test$dfSummary %>%
      select("GroupID", "Flag"), by = "GroupID")

  expect_identical(flags$hardcode_flag, flags$Flag.x)
  expect_identical(flags$hardcode_flag, flags$Flag.y)
})
```

# Qualification Report

The Qualification Report is generated to document and display the qualification that the code has been through.  The report lives as a Qualification vignette in `{gsm}` and is rendered during other workflows.  The Qualification Report is also attached to each release and included in the `{pkgdown}` site to display the qualification status of `{gsm}`.  The sections of the Qualification Report are outlined below.

### Qualification Testing Results

Using the specifications, test cases, and test code outlined above the qualification status of all assessments currently qualified within `{gsm}` is rendered, consisting of smaller sections for each assessment.  These smaller sections will include the procedure that is being qualified, which should correspond to the function that is used for that procedure.  An overview of the specifications is also included that has the ID, Description, Risk Level, Risk Impact, and associated test cases corresponding to each specification.  This information is pulled from the Specification Spreadsheet file (`qualification_specs.csv`) outlined above.  

#### Test Results: Overview

An overview of the qualification test results is presented as a table, with one row for each function that has been tested. The results are presented as a series of columns for the number of tests, number of passing tests, number of failing tests, and number of skipped tests.  

```{r results = "hide"}
spec_list <- list.files(here::here('tests', 'testqualification', 'qualification'), pattern = "test-qual_", full.names = TRUE)

scrape <- map_df(spec_list, function(x) {
    testResultsRaw <- testthat::test_file(x, reporter = testthat::ListReporter, package = "gsm") %>% 
      as_tibble()
    
    return(testResultsRaw)
}) %>% 
  mutate(Tests = gsub("test-qual_", "", file),
         Tests = gsub(".R", "", Tests))

funcs <- spec_df %>%
  select(Tests, Assessment) %>%
  tidyr::separate_rows("Tests", sep = ",") %>%
  mutate(Tests = trimws(Tests)) %>%
  rename(Function = Assessment) %>%
  distinct()

testResultsIndividual <- left_join(scrape, funcs, by = "Tests") %>% 
  group_by(Function) %>%
  reframe(`Function Name` = unique(Function),
          `Number of Tests` = sum(nb),
          `Number Passed` = sum(passed),
          `Number Failed` = sum(failed),
          `Number Skipped` = sum(skipped)) %>%
  arrange(`Function Name`) %>%
  select(-Function)

scrape_new <- left_join(scrape, funcs, by = "Tests") 
```

```{r}
testResultsIndividual %>%
  gt() %>%
  tab_options(
    table.font.size = px(9),
    table.width = pct(80)  # Adjust width as needed
  ) %>% 
  opt_row_striping() 
``` 

#### Test Results: Detailed  

A detailed summary of the qualification test results is also provided in table format in the Qualification Report. In this section, two tables are presented, where the first presents each row as corresponding to a single specification and the second presents each row as corresponding to a single test. 

- **One Row Per Specification** - Each row corresponds to a specification, and each specification is presented with a general description of the functionality tested for each specification, along with risk level, risk impact, and associated test IDs. In most cases, there are multiple test IDs associated with each specification.  

```{r}
qual_specs_orig <- read.csv(system.file("qualification", "qualification_specs.csv", package = "gsm"))
qual_specs <- qual_specs_orig %>%
  mutate(SpecID = paste0("S", Spec, "_", Test.ID)) %>%
  select(Specs = SpecID, Tests, N.Tests)

### One row per spec
Specifications <- spec_df %>%
  select(ID, Description, Risk, Impact, Tests) %>%
  mutate(sort1 = as.numeric(str_extract(ID, "(?<=S)\\d+")),
         sort2 = as.numeric(str_extract(ID, "\\d+$"))) %>%
  arrange(as.numeric(sort1), as.numeric(sort2)) %>%
  select(-c(sort1, sort2))
colnames(Specifications) <- c("Spec ID", "Spec Description", "Risk", "Impact", "Associated Test IDs")

Specifications %>%
  gt() %>%
  tab_options(
    table.font.size = px(9)
  )%>%
  cols_width(
    `Spec Description` ~ px(180), 
    `Associated Test IDs` ~ px(150) 
  ) %>%
  opt_row_striping() 
```  

- **One Row Per Test** - Each row corresponds to a single test (Test ID), and each test is presented with the function which is tested, the specification IDs the test satisfies, the detailed test description (including grouping variables and other function arguments that are tested), and the result of the test (Pass/Fail/Skip).  

```{r}
### One row per test
test <- scrape_new %>%
  mutate(result_new = ifelse(failed == 0, "Pass", "Fail")) %>%
  select(Tests, test, result_new, Function)

Results <- left_join(test, qual_specs, by = "Tests") %>%
  select(Function, Specs, Tests, test, result_new) %>%
  mutate(sort1 = as.numeric(str_extract(Tests, "(?<=T)\\d+")),
         sort2 = as.numeric(str_extract(Tests, "\\d+$"))) %>%
  arrange(as.numeric(sort1), as.numeric(sort2)) %>%
  select(-c(sort1, sort2))
colnames(Results) <- c("Function", "Spec ID", "Test ID", "Test Description", "Test Result")

Results %>%
  gt() %>%
  tab_options(
    table.font.size = px(9)
  ) %>%
  cols_width(
    `Function` ~ px(120), 
    `Spec ID` ~ px(90),
    `Test Description` ~ px(180)
  ) %>%
  opt_row_striping() 
```  

### Unit Tests

A summary of the unit test coverage is included in the Qualification Report to show how well the package functions are unit tested.  This is created by using `covr::package_coverage()` and then listed out by function. Unit testing is performed in addition to qualification testing to help ensure that individual pieces of code within the R package function correctly and produce the expected results. By testing individual units of code in isolation, developers can identify and fix issues early in the development process before more significant and scaled problems arise. 


### Qualification Testing Environment

The `sessionInfo()` of the qualification environment is included to show what R version, platform, and packages were used when running the Qualification Report. This is called after all necessary packages have been loaded and all setup is done. The environment should not change after this part of the report is created. In addition, a package list is provided, which includes the package version and package score from `riskmetric`, which quantifies the robustness of an R package. The `pkg_score` column captures the risk involved with using a package. The risk level ranges from 0 (low risk) to 1 (high risk).  


### Pull Requests

The final section of the Qualification Report is an overview of all Pull Requests since the last release. This includes the title, compare and base branches, a link to the GitHub page, requester, reviewers, date requested, and the status of the Pull Request.  While this is meant to be a comprehensive overview of the Pull Requests the release documentation should also include links to all Pull Requests included in the release.
