---
title: "Cookbook"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Cookbook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(gsm)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This vignette contains a series of examples showing how to run analysis workflows for the Good Statistical Monitoring `{gsm}` package using sample data from [`{clindata}`](https://github.com/Gilead-BioStats/clindata). 

`{gsm}` leverages Key Risk Indicators (KRIs) and thresholds to conduct study-level and site-level Risk Based Monitoring for clinical trials.

For more information on the `{gsm}` package see the [package homepage](https://gilead-biostats.github.io/gsm/). The [Data Pipeline Vignette](https://gilead-biostats.github.io/gsm/articles/DataPipeline.html) provides additional technical details including data specifications and handling of metadata. The [Data Analysis Vignette](https://gilead-biostats.github.io/gsm/articles/DataAnalysis.html) provides detailed information about the nested functions that support the analysis workflows.

## Setup and Installation

### Install `{gsm}`, `{clindata}`, and `{safetyData}`

Run the following:

```{r eval = FALSE, include = TRUE}
## Install devtools
install.packages('devtools')

## Install and load sample raw data
devtools::install_github("Gilead-BioStats/clindata", ref = "main")
library(clindata)

## Install and load sample CDISC-compliant SDTM and ADaM data
install.packages('safetyData')
library(safetyData)

## Install and load gsm
devtools::install_github("Gilead-BioStats/gsm", ref = "main")
library(gsm)
```

To use the most recent development version from GitHub, run:

```{r eval = FALSE, include = TRUE}
devtools::install_github("Gilead-BioStats/gsm", ref = "dev")
```

## Data Prep Using Sample Data from `{clindata}`

To run multiple assessments using the sample rawplus data from `{clindata}`, the user can run the following:

It is important to note that `lData` expects a *named* list. To see a list of default data frame names, run `names(gsm::Read_Mapping())`.

```{r eval = FALSE, include = TRUE}
library(gsm)
library(clindata)
library(dplyr)

# Import Site+Study Metadata
dfStudy<-clindata::ctms_study %>% rename(StudyID = protocol_number)
dfSite<- clindata::ctms_site %>% rename(SiteID = site_num)

# Pull Raw Data - this will overwrite the previous data pull
lRaw <- gsm::UseClindata(
  list(
    "dfSUBJ" = "clindata::rawplus_dm",
    "dfAE" = "clindata::rawplus_ae",
    "dfPD" = "clindata::ctms_protdev",
    "dfLB" = "clindata::rawplus_lb",
    "dfSTUDCOMP" = "clindata::rawplus_studcomp",
    "dfSDRGCOMP" = "clindata::rawplus_sdrgcomp %>%
      dplyr::filter(.data$phase == 'Blinded Study Drug Completion')",
    "dfDATACHG" = "clindata::edc_data_points",
    "dfDATAENT" = "clindata::edc_data_pages",
    "dfQUERY" = "clindata::edc_queries",
    "dfENROLL" = "clindata::rawplus_enroll"
  )
)
```

Once the data is loaded into the R session, we can begin running our workflows to produce analytics and reporting outputs.

## Example 1 - Running Assessments on Standard Data with YAML files

All assessments can be run as standalone workflows that create useful data summaries and visualizations. `{gsm}` is configured to accept rawplus data by default, and that data standard is used here. If using YAML files to specify the workflow(s), the process for running one assessment or multiple assessments is identical.

With all necessary data loaded into the session, running assessments is a 3-step process, both using the same `RunWorkflow()` function. The steps are as follows:

1.  Prepare the workflow lists with `MakeWorkflowList()`
2.  Map domain-level data to the input data standard for the assessment(s)
3.  Run the assessment(s) using an appropriate statistical method as specified in the YAML file

First, we must convert the YAML files into usable lists in R with the use of `MakeWorkflowList()`. This function takes a directory path `strPath` and an optional file name(s) argument `strNames`. There is also a `bRecursive` logical argument that specifies whether or not you want the function to find files in nested folder (default is `FALSE`). The output of this step is a named list of workfloes with workflow and parameter metadata. Running `MakeWorkflowList()` with no arguments specified will make a workflow list from the yaml files in the `/inst/workflows` directory of the `{gsm}` package.

Second, we will create the mapping data to be used when creating the Metrics in the next step. The `mapping.yaml` workflow creates the necessary `data.frames` for use in the default assessment workflows. Now that the workflow lists are prepared from the previous step, running `RunWorkflow(lWorkflow = wf_mapping, lData = lRaw)` will produce a list containing all raw data `$lData` and mapped data `$lResult` for use in calculating metrics in the next step.

This YAML file runs a series of `RunQuery()` calls to create the appropriately filtered and specified data.frames. The full `mapping.yaml` workflow is below, in Appendix A. Here, you will see a subset of the `mapping.yaml` that is relevant to the Important Protocol Deviation Rate workflow (KRI0004), which is the example we will focus on throughout this vignette. 


    meta:
      file: mapping.yaml
      desctipion: Data Mappings to prepare for metric calculations
    steps:
      - name: RunQuery
        output: dfEnrolled
        params:
          df: dfSUBJ
          strQuery: "SELECT subjectid as raw_subjectid, * FROM df WHERE enrollyn == 'Y'"
      - name: RunQuery
        output: dfImportantPD
        params:
          df: dfPD
          strQuery: "SELECT subjectenrollmentnumber as subjid, * FROM df WHERE deemedimportant == 'Yes'"


The final step is to run the assessment workflow, which uses `RunWorkflow()` again, as we did in the previous step. Each KRI has its own yaml workflow file, but an example of the PD KRI workflow is as follows:


    meta:
      file: kri0004.yaml
      MetricID: kri0004
      group: site
      abbreviation: PD
      metric: Important Protocol Deviation Rate
      numerator: Important Protocol Deviations
      denominator: Days on Study
      model: Normal Approximation
      score: Adjusted Z-Score
    vThreshold:
      - -3
      - -2
      - 2
      - 3
    steps:
      - name: Input_Rate
        output: dfInput
        params:
          dfSubjects: dfEnrolled
          dfNumerator: dfImportantPD
          dfDenominator: dfEnrolled
          strSubjectCol: subjid
          strGroupCol: siteid
          strNumeratorMethod: Count
          strDenominatorMethod: Sum
          strDenominatorCol: timeonstudy
      - name: Transform_Rate
        output: dfTransformed
        params:
         dfInput: dfInput
      - name: Analyze_NormalApprox
        output: dfAnalyzed
        params:
          dfTransformed: dfTransformed
          strType: rate
      - name: Analyze_NormalApprox_PredictBounds
        output: dfBounds
        params:
          dfTransformed: dfTransformed
          strType: rate
          vThreshold: vThreshold
      - name: Flag_NormalApprox
        output: dfFlagged
        params:
          dfAnalyzed: dfAnalyzed
          vThreshold: vThreshold
      - name: Summarize
        output: dfSummary
        params:
          dfFlagged: dfFlagged


Putting these three steps together to run all 12 KRI workflows at once looks like this:

```{r eval = FALSE, include = TRUE}
library(clindata)
library(gsm)

# Prepare Workflows
wf_mapping <- MakeWorkflowList(strNames="mapping")
wf_metrics <- MakeWorkflowList(strNames=paste0("kri",sprintf("%04d", 1:12)))

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, 
                       lData = lRaw)
# Run KRI workflows
lResults <- RunWorkflow(lWorkflow = wf_metrics, 
                        lData = lMapped$mapping$lResults)
```


## Example 2 - Running a Single Assessment Step-by-Step Using Standard Data

Let's look at an illustrative example of everything that happens behind the scenes when running through a YAML file via the `RunWorkflow()` function. It may be helpful to reference this example alongside the [Data Analysis Vignette](https://gilead-biostats.github.io/gsm/articles/DataAnalysis.html), which goes into significantly more detail about each nested function involved in this workflow. The image below is a high-level summary:

![](data-analysis.PNG)

As displayed in the YAML files of Example 1, we will use the PD workflow (KRI0004) for this example. OR USE RUNQUERY()!!

```{r eval = FALSE, include = TRUE}
library(clindata)
library(gsm)

# Prepare Workflow
wf_mapping <- MakeWorkflowList(strNames="mapping")

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, 
                       lData = lRaw)
```

Start by creating `dfInput` using sample rawplus data from `{clindata}`. Note that `Input_Rate()` requires three specific datasets from `{clindata}`, which include a subject-level demographics/exposure dataset (`dfSubjects`) and a domain-level dataset (`dfNumerator`) that records every adverse event per subject. 

Since `Input_Rate()` is a generalized function, it is also required that you specify the relevant column names for the Subject (`strSubjectCol`), Group (`strGroupCol`) and optionally the Denominator (`strDenominatorCol`) and Numerator (`strNumeratorCol`) when it is not simply "Denominator" or "Numerator", respectively.

Finally, the method for calculating the Numerator and Denominator is specified in `strNumeratorMethod` and `strDenominatorMethod` as either "Count" or "Sum".  If the method is "Count", the function simply counts the number of rows in the provided data frame. If the numerator method is "Sum", the function takes the sum of the values in the specified column (strNumeratorCol or strDenominatorCol). The function returns a data frame with the calculated input rate for each subject.

```{r eval = FALSE, include = TRUE}
dfInput <- Input_Rate(
              dfSubjects =  lMapped$mapping$lResults$dfEnrolled
              dfNumerator = lMapped$mapping$lResults$dfImportantPD
              dfDenominator = lMapped$mapping$lResults$dfEnrolled
              strSubjectCol = "subjid"
              strGroupCol = "siteid"
              strNumeratorMethod = "Count"
              strDenominatorMethod = "Sum"
              strDenominatorCol = "timeonstudy"
)
```

The second step is to transform `dfInput` into site-level summary data, which is needed to derive the KRI. Two `Transform()` functions are available, `Transform_Rate()` and `Transform_Count()`, and `Transform_Rate()` is the function that is used with the PD assessment. Note that `strNumeratorCol`and `strDenominatorCol` are optional, with the default values being "Numerator" and "Denominator".

```{r eval = FALSE, include = TRUE}
dfTransformed <- Transform_Rate(dfInput)
```

The third step is to analyze the transformed data. Certain assessments support multiple statistical models/`Analyze()` functions, so the user needs to specify certain parameters for the intended analysis to run. We use the normal approximation method in this example (the default method for all standard KRIs).

```{r eval = FALSE, include = TRUE}
dfAnalyzed <- Analyze_NormalApprox(dfTransformed)
```

After analyzing the transformed data, we will flag sites based on at least one statistical threshold. Similar to the previous step, multiple flag functions exist to support the assessment functions. The `Flag_NormalApprox()` function is one of several options for the PD assessment. 

- If a site falls outside of the (-2, 2) range, it is considered to be "Amber" and will be assigned a value of -1 or 1. 
- If a site falls outside of the (-3, 3) range, it is considered "Red" and will be assigned a value of -2 or 2. 

Both are considered to be outliers, but "Red" sites are more extreme outliers.

```{r eval = FALSE, include = TRUE}
dfFlagged <- Flag_NormalApprox(dfAnalyzed, vThreshold = c(-3, -2, 2, 3))
```

Next, we will summarize the flagged data. The `Summarize()` function will only keep the most relevant columns from `dfFlagged` for a given assessment.

```{r eval = FALSE, include = TRUE}
dfSummary <- Summarize(dfFlagged)
```

Finally, we will create a chart that helps to identify/visualize flagged sites. When using the normal approximation method, we can create upper and lower bounds using the `Analyze_NormalApprox_PredictBounds()` function. 

Using the bounds we create and the flagged data frame, we can create a scatter plot. Any sites that are plotted outside of the bounds will be yellow or red. Yellow points indicate "At Risk" sites, and red points indicate "Flagged" sites.

```{r eval = FALSE, include = TRUE}
dfBounds <- Analyze_NormalApprox_PredictBounds(dfTransformed, vThreshold = c(-3, -2, 2, 3))
chart <- Visualize_Scatter(dfFlagged, dfBounds)
```


Reviewing this step-by-step example will allow the user to better understand each intermediate function that runs within a YAML workflow file during KRI derivation. `{gsm}` allows the user to generate the same results in Example 1 and Example 2, with additional error-checking and metadata, and interactive data visualizations. For reference, the code from Example 1 is specified below, only running the PD workflow (KRI0004):

```{r eval = FALSE, include = TRUE}
# Prepare Workflows
wf_mapping <- MakeWorkflowList(strNames = "mapping")
wf_metrics <- MakeWorkflowList(strNames = "kri0004")

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, 
                       lData = lRaw)
# Run KRI workflows
lResults <- RunWorkflow(lWorkflow = wf_metrics, 
                        lData = lMapped$mapping$lResults)
```

# Visualizations

To create all visualization for a given workflow (or list of workflows), the `Visualize_Metric()` returns a list with all relevant visualizations for each metric. In order to get a list of charts for all metrics in the pipeline, the following script can be run using the data frames returned in `lResults` in the examples above.

```{r eval = FALSE, include = TRUE}
lCharts <- unique(dfSummary$MetricID) %>% map(~Visualize_Metric(
  dfSummary = dfSummary, 
  dfBounds = dfBounds, 
  dfSite = dfSite,
  dfMetrics = dfMetrics, 
  strMetricID = .x
)
) %>% setNames(unique(dfSummary$MetricID)) 
```

Below we will walk through 

## Example 3 - Creating a Scatter Plot

`{gsm}` creates static and interactive charts using `{ggplot2}` and a custom JavaScript charting library `{rbm-viz}`. 

Customizing interactive JavaScript widgets will be covered in a separate vignette.

Below is an example of creating a static `{ggplot2}` scatter plot that uses the default normal approximation method for PD Metrics and rawplus data from `{clindata}`:

```{r eval = FALSE, include = TRUE}
library(gsm)
library(clindata)

# Prepare Workflows
wf_mapping <- MakeWorkflowList(strNames="mapping")[[1]]
wf_metrics <- MakeWorkflowList(strNames=paste0("kri",sprintf("%04d", 1:2)))
dfMetrics <- wf_metrics %>% map_df(~.x$meta)

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, lData = lRaw)$lData

# Run Metrics
lResults <- RunWorkflow(lWorkflow=wf_metrics, lData=lMapped)

# Update dfBounds and dfSummary with StudyID and SnapshotDate
dfBounds <- lResults %>% 
  imap_dfr(~.x$lResults$dfBounds %>% mutate(MetricID = .y)) %>%
  mutate(StudyID = "ABC-123") %>%
  mutate(SnapshotDate = Sys.Date())

dfSummary <- lResults %>% 
  imap_dfr(~.x$lResults$dfSummary %>% mutate(MetricID = .y)) %>%
  mutate(StudyID = "ABC-123") %>%
  mutate(SnapshotDate = Sys.Date())

# Create the visualization
Visualize_Scatter(
  dfSummary = dfSummary, 
  dfBounds = dfBounds
  )
```


# Reporting

## Example 4 - Creating an Assessment Overview Report for Multiple Assessments

### Standard Report via `Report_KRI()`

The `Report_KRI()` function creates the **Assessment Overview Report**, which is an HTML document that contains tables, visualizations, and error logging for all assessments run in the `Run_Workflow()` workflow.

Let's create a report using sample data from `{clindata}`:

```{r eval = FALSE, include = TRUE}
library(gsm)
library(clindata)

# Prepare Workflows
wf_mapping <- MakeWorkflowList(strNames="mapping")[[1]]
wf_metrics <- MakeWorkflowList(strNames=paste0("kri",sprintf("%04d", 1:2)))
dfMetrics <- wf_metrics %>% map_df(~.x$meta)

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, lData = lRaw)$lData

# Run Metrics
lResults <- RunWorkflow(lWorkflow=wf_metrics, lData=lMapped)

# Update dfBounds and dfSummary with StudyID and SnapshotDate
dfBounds <- lResults %>% 
  imap_dfr(~.x$lResults$dfBounds %>% mutate(MetricID = .y)) %>%
  mutate(StudyID = "ABC-123") %>%
  mutate(SnapshotDate = Sys.Date())

dfSummary <- lResults %>% 
  imap_dfr(~.x$lResults$dfSummary %>% mutate(MetricID = .y)) %>%
  mutate(StudyID = "ABC-123") %>%
  mutate(SnapshotDate = Sys.Date())

# Generate Report
strOutpath <- "~/gsm_site_report_db.html"
Report_KRI( lCharts = lCharts, 
            dfSummary = dfSummary,  
            dfSite = dfSite, 
            dfStudy = dfStudy, 
            dfMetrics =dfMetrics, 
            strOutpath = strOutpath )
```

The report will render and be saved to your current working directory. If you would like the report saved in a different location, you can set an output directory using the `strOutpath` argument in the `Study_Report()` function.

# Custom Mapping/YAML Specifications

As mentioned in Appendix 1 of the [Data Pipeline Vignette](https://gilead-biostats.github.io/gsm/articles/DataPipeline.html#Appendix-1-Metadata-Technical-Specifications), `Study_Assess()` triggers a workflow that uses pre-defined YAML specifications that organize all of the required metadata for a given assessment or set of assessments.

In some cases, the user might want to configure their own mappings, which can be done by providing custom YAML mappings for one or more assessments.

Let's take a look at a few examples of editing YAML files for a custom workflow below.

## Example 8 - Adding a Filtering Step

There are filter steps in the default specification for the AE Assessment, but we want to add an additional filter. Let's add a step that filters the `dfImportantPD` data frame so all values of `companycategory == "WRONG TREATMENT OR INCORRECT DOSE"`:

The default mappings for the PD Assessment are saved under KRI 0004, i.e., `kri0004.yaml`:

    meta:
      file: kri0004.yaml
      MetricID: kri0004
      group: site
      abbreviation: PD
      metric: Important Protocol Deviation Rate
      numerator: Important Protocol Deviations
      denominator: Days on Study
      model: Normal Approximation
      score: Adjusted Z-Score
    vThreshold:
      - -3
      - -2
      - 2
      - 3
    steps:
      - name: Input_Rate
        output: dfInput
        params:
          dfSubjects: dfEnrolled
          dfNumerator: dfImportantPD_wrong_trt
          dfDenominator: dfEnrolled
          strSubjectCol: subjid
          strGroupCol: siteid
          strNumeratorMethod: Count
          strDenominatorMethod: Sum
          strDenominatorCol: timeonstudy
      - name: Transform_Rate
        output: dfTransformed
        params:
         dfInput: dfInput
      - name: Analyze_NormalApprox
        output: dfAnalyzed
        params:
          dfTransformed: dfTransformed
          strType: rate
      - name: Analyze_NormalApprox_PredictBounds
        output: dfBounds
        params:
          dfTransformed: dfTransformed
          strType: rate
          vThreshold: vThreshold
      - name: Flag_NormalApprox
        output: dfFlagged
        params:
          dfAnalyzed: dfAnalyzed
          vThreshold: vThreshold
      - name: Summarize
        output: dfSummary
        params:
          dfFlagged: dfFlagged

First, we want to add an additional `RunQuery()` step to the workflow specification to select only PDs categorized as "WRONG TREATMENT OR INCORRECT DOSE". Because `dfImportantPD` needs to be filtered before it is used in `Input_Rate()`, the filter steps need to be placed first in the workflow.

Let's name the custom YAML file `kri0004_filtered.yaml`. It reads as follows:

    meta:
      file: kri0004.yaml
      MetricID: kri0004
      group: site
      abbreviation: PD
      metric: Important Protocol Deviation Rate
      numerator: Important Protocol Deviations
      denominator: Days on Study
      model: Normal Approximation
      score: Adjusted Z-Score
    vThreshold:
      - -3
      - -2
      - 2
      - 3
    steps:
     - name: RunQuery
        output: dfImportantPD_wrong_trt
        params:
          df: dfImportantPD
          strQuery: "SELECT * FROM df WHERE companycategory == 'WRONG TREATMENT OR INCORRECT DOSE'"
      - name: Input_Rate
        output: dfInput
        params:
          dfSubjects: dfEnrolled
          dfNumerator: dfImportantPD
          dfDenominator: dfEnrolled
          strSubjectCol: subjid
          strGroupCol: siteid
          strNumeratorMethod: Count
          strDenominatorMethod: Sum
          strDenominatorCol: timeonstudy
      - name: Transform_Rate
        output: dfTransformed
        params:
         dfInput: dfInput
      - name: Analyze_NormalApprox
        output: dfAnalyzed
        params:
          dfTransformed: dfTransformed
          strType: rate
      - name: Analyze_NormalApprox_PredictBounds
        output: dfBounds
        params:
          dfTransformed: dfTransformed
          strType: rate
          vThreshold: vThreshold
      - name: Flag_NormalApprox
        output: dfFlagged
        params:
          dfAnalyzed: dfAnalyzed
          vThreshold: vThreshold
      - name: Summarize
        output: dfSummary
        params:
          dfFlagged: dfFlagged

Now that this new workflow has been saved with a unique file name, we can call it with `MakeWorkflowList()` as above

```{r eval = FALSE, include = TRUE}
library(gsm)
library(clindata)
library(yaml)

## Read in workflow mapping
wf_mapping <- MakeWorkflowList(strNames = "mapping")
wfmetrics_custom <- MakeWorkflowList("kri0004_filtered")
dfMetrics <- wf_metrics %>% map_df(~.x$meta)

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, lData = lRaw)$lData

# Run Metrics
lResults <- RunWorkflow(lWorkflow=wfmetrics_custom, lData=lMapped)
```


# Appendix A: Full `mapping.yaml` file specification

The following YAML file creates all modified rawplus dataframes needed to run all default workflows specified in the `{gsm}` package. Additional `RunQuery()` steps can be added to this file for additional user-defined workflows.

    meta:
      file: mapping.yaml
      description: Data Mappings to prepare for metric calculations
    steps:
      - name: RunQuery
        output: dfEnrolled
       params:
         df: dfSUBJ
         strQuery: "SELECT subjectid as raw_subjectid, * FROM df WHERE enrollyn == 'Y'"
     - name: RunQuery
       output: dfSeriousAE
        params:
         df: dfAE
         strQuery: "SELECT * FROM df WHERE aeser = 'Y'"  
     - name: RunQuery
        output: dfNonimportantPD
        params:
          df: dfPD
          strQuery: "SELECT subjectenrollmentnumber as subjid, * FROM df WHERE deemedimportant == 'No'"
      - name: RunQuery
        output: dfImportantPD
        params:
          df: dfPD
          strQuery: "SELECT subjectenrollmentnumber as subjid, * FROM df WHERE deemedimportant == 'Yes'"
      - name: RunQuery
        output: dfAllLabs
        params:
          df: dfLB
          strQuery: "SELECT * FROM df WHERE toxgrg_nsv IN ('0', '1', '2', '3', '4')"
      - name: RunQuery
        output: dfToxLabs
        params:
          df: dfLB
          strQuery: "SELECT * FROM df WHERE toxgrg_nsv IN ('3', '4')"
      - name: RunQuery
        output: dfStudyDropouts
        params:
          df: dfSTUDCOMP
          strQuery: "SELECT * FROM df WHERE compyn IN ('N')"  
      - name: RunQuery
        output: dfTreatmentDropouts
        params:
          df: dfSDRGCOMP
          strQuery: "SELECT * FROM df WHERE sdrgyn IN ('N') AND phase = 'Blinded Study Drug Completion'"  
      - name: RunQuery
        output: dfValidQueries
        params:
          df: dfQUERY
          strQuery: "SELECT subjectname as subject_nsv, * FROM df WHERE querystatus IN ('Open', 'Answered', 'Closed')"
      - name: RunQuery
        output: dfOldValidQueries
        params:
          df: dfValidQueries
          strQuery: "SELECT * FROM df WHERE queryage > 30"  
      - name: RunQuery
        output: dfDataChanges
        params:
          df: dfDATACHG
          strQuery: "SELECT subjectname as subject_nsv, * FROM df"    
      - name: RunQuery
        output: dfQuery
        params:
          df: dfQUERY
          strQuery: "SELECT subjectname as subject_nsv, * FROM df"  
      - name: RunQuery
        output: dfDataEntry
        params:
          df: dfDATAENT
          strQuery: "SELECT subjectname as subject_nsv, * FROM df"
      - name: RunQuery
        output: dfSlowDataEntry
        params:
          df: dfDATAENT
          strQuery: "SELECT subjectname as subject_nsv, * FROM df WHERE data_entry_lag > 10"  
      - name: RunQuery
        output: dfChangedDataPoints
        params:
          df: dfDATACHG
          strQuery: "SELECT subjectname as subject_nsv, * FROM df WHERE n_changes > 0"  
      - name: RunQuery
        output: dfScreened
        params:
          df: dfENROLL
          strQuery: "SELECT * FROM df"
      - name: RunQuery
        output: dfScreenFail
        params:
          df: dfENROLL
          strQuery: "SELECT * FROM df WHERE enrollyn = 'N'" 

