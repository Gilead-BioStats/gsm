---
title: "Cookbook"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Cookbook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(gsm)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This vignette contains a series of examples showing how to run analysis workflows for the Good Statistical Monitoring `{gsm}` package using sample data from [`{clindata}`](https://github.com/Gilead-BioStats/clindata). 

`{gsm}` leverages Key Risk Indicators (KRIs) and thresholds to conduct study-level and site-level Risk Based Monitoring for clinical trials.

For more information on the `{gsm}` package see the [package homepage](https://gilead-biostats.github.io/gsm/). The [Data Pipeline Vignette](https://gilead-biostats.github.io/gsm/articles/DataPipeline.html) provides additional technical details including data specifications and handling of metadata. The [Data Analysis Vignette](https://gilead-biostats.github.io/gsm/articles/DataAnalysis.html) provides detailed information about the nested functions that support the analysis workflows.

## Setup and Installation

### Install `{gsm}`, `{clindata}`, and `{safetyData}`

Run the following:

```{r eval = FALSE, include = TRUE}
## Install devtools
install.packages('devtools')

## Install and load sample raw data
devtools::install_github("Gilead-BioStats/clindata", ref = "main")
library(clindata)

## Install and load sample CDISC-compliant SDTM and ADaM data
install.packages('safetyData')
library(safetyData)

## Install and load gsm
devtools::install_github("Gilead-BioStats/gsm", ref = "main")
library(gsm)
```

To use the most recent development version from GitHub, run:

```{r eval = FALSE, include = TRUE}
devtools::install_github("Gilead-BioStats/gsm", ref = "dev")
```


## Example 1 - Running an Assessment on Standard Data

Below is the script for running an AE assessment corresponding to kri0001. Using Raw+ data from `clindata`, we are able to create the mapped input data (via `InputRate()`), prepare that data for analysis (via `Transform_Rate()`), analyze the data with the Normal Approximation method (via `Analyze_NormalApprox()`) and flag and summarize the analyzed data (via `Flag_NormalApprox()` and `Summarize()`, respectively) in order to produce a table which aggregates site level flags for the AE risk signal.


```{r code = readLines(system.file("examples", "1_1_workflow_basic.R", package = "gsm")), eval = FALSE, include = TRUE}

```

This can also be done with pipes, if desired.

```{r code = readLines(system.file("examples", "1_2_workflow_pipes.R", package = "gsm")), eval = FALSE, include = TRUE}

```

## Example 2 - Running a Single Assessment with Charts and Report

In order to create charts and a report for the AE assessment, a few additional steps must be taken. First, `Analyze_NormalApprox_PredictBounds()` must be run to create a data.frame with the upper/lower bounds of the prections (`dfBounds`). Then, a data.frame with all relevant meta information is constructed (`dfMetrics`) prior to running `Visualize_Metric()` and `Report_KRI()` which create the charts and report, respectively.


```{r code = readLines(system.file("examples", "1_3_workflow_report.R", package = "gsm")), eval = FALSE, include = TRUE}

```

# Using YAML files to run workflows

All assessments can be specified with YAML files, which run through the process explained in the previous section using the follwing three steps:
  
1.  Prepare the workflow lists with `MakeWorkflowList()`
2.  Map domain-level data to the input data standard for the assessment(s) with `RunWorkflow()`.
3.  Run the assessment(s) using an appropriate statistical method as specified in the YAML file with `RunWorkflow`.

## Example 3 - YAML workflow for AE KRI

Here, we run the YAML workflow for the AE KRI (kri0001). The table produced at the end of this script will match the tables produced in Example 1 above.


```{r code = readLines(system.file("examples", "2_1_workflow_yaml_basic.R", package = "gsm")), eval = FALSE, include = TRUE}

```

## Example 4 - Generate Charts with yaml workflow for multiple KRIs

In this example, we run multiple KRIs and once by feeding a vector of KRI file names into `MakeWorkflowList()`. Then, to make charts, we add study information to `dfBounds` and `dfSummary` and feed all relevant data.frames into the `Visualize_Metric()` function using `purrr:map()` since we are making charts for all 12 KRIs. 


```{r code = readLines(system.file("examples", "2_2_workflow_yaml_charts.R", package = "gsm")), eval = FALSE, include = TRUE}

```

## Example 5: Generate Site Report with yaml workflow for multiple KRIs

Here, we create the site-level report using `Report_KRI()` with the data.frames produced by the yaml workflow and the charts created with `Visualize_Metric()`. 

```{r code = readLines(system.file("examples", "2_3_workflow_yaml_country.R", package = "gsm")), eval = FALSE, include = TRUE}

```

## Example 6: Generate Country Report with yaml workflow for multiple KRIs

Using the same process as the site-level report, we create a country-level report by using the Country KRI yaml files, which begin with `cou` instead of `kri`.

```{r code = readLines(system.file("examples", "2_4_workflow_yaml_report.R", package = "gsm")), eval = FALSE, include = TRUE}

```

# Appendix: Custom Mapping/YAML Specifications

As mentioned in Appendix 1 of the [Data Pipeline Vignette](https://gilead-biostats.github.io/gsm/articles/DataPipeline.html#Appendix-1-Metadata-Technical-Specifications), `Study_Assess()` triggers a workflow that uses pre-defined YAML specifications that organize all of the required metadata for a given assessment or set of assessments.

In some cases, the user might want to configure their own mappings, which can be done by providing custom YAML mappings for one or more assessments.

Let's take a look at a few examples of editing YAML files for a custom workflow below.

## Example 8 - Adding a Filtering Step

There are filter steps in the default specification for the AE Assessment, but we want to add an additional filter. Let's add a step that filters the `dfImportantPD` data frame so all values of `companycategory == "WRONG TREATMENT OR INCORRECT DOSE"`:

The default mappings for the PD Assessment are saved under KRI 0004, i.e., `kri0004.yaml`:

    meta:
      file: kri0004.yaml
      MetricID: kri0004
      group: site
      abbreviation: PD
      metric: Important Protocol Deviation Rate
      numerator: Important Protocol Deviations
      denominator: Days on Study
      model: Normal Approximation
      score: Adjusted Z-Score
    vThreshold:
      - -3
      - -2
      - 2
      - 3
    steps:
      - name: Input_Rate
        output: dfInput
        params:
          dfSubjects: dfEnrolled
          dfNumerator: dfImportantPD_wrong_trt
          dfDenominator: dfEnrolled
          strSubjectCol: subjid
          strGroupCol: siteid
          strNumeratorMethod: Count
          strDenominatorMethod: Sum
          strDenominatorCol: timeonstudy
      - name: Transform_Rate
        output: dfTransformed
        params:
         dfInput: dfInput
      - name: Analyze_NormalApprox
        output: dfAnalyzed
        params:
          dfTransformed: dfTransformed
          strType: rate
      - name: Analyze_NormalApprox_PredictBounds
        output: dfBounds
        params:
          dfTransformed: dfTransformed
          strType: rate
          vThreshold: vThreshold
      - name: Flag_NormalApprox
        output: dfFlagged
        params:
          dfAnalyzed: dfAnalyzed
          vThreshold: vThreshold
      - name: Summarize
        output: dfSummary
        params:
          dfFlagged: dfFlagged

First, we want to add an additional `RunQuery()` step to the workflow specification to select only PDs categorized as "WRONG TREATMENT OR INCORRECT DOSE". Because `dfImportantPD` needs to be filtered before it is used in `Input_Rate()`, the filter steps need to be placed first in the workflow.

Let's name the custom YAML file `kri0004_filtered.yaml`. It reads as follows:

    meta:
      file: kri0004.yaml
      MetricID: kri0004
      group: site
      abbreviation: PD
      metric: Important Protocol Deviation Rate
      numerator: Important Protocol Deviations
      denominator: Days on Study
      model: Normal Approximation
      score: Adjusted Z-Score
    vThreshold:
      - -3
      - -2
      - 2
      - 3
    steps:
     - name: RunQuery
        output: dfImportantPD_wrong_trt
        params:
          df: dfImportantPD
          strQuery: "SELECT * FROM df WHERE companycategory == 'WRONG TREATMENT OR INCORRECT DOSE'"
      - name: Input_Rate
        output: dfInput
        params:
          dfSubjects: dfEnrolled
          dfNumerator: dfImportantPD
          dfDenominator: dfEnrolled
          strSubjectCol: subjid
          strGroupCol: siteid
          strNumeratorMethod: Count
          strDenominatorMethod: Sum
          strDenominatorCol: timeonstudy
      - name: Transform_Rate
        output: dfTransformed
        params:
         dfInput: dfInput
      - name: Analyze_NormalApprox
        output: dfAnalyzed
        params:
          dfTransformed: dfTransformed
          strType: rate
      - name: Analyze_NormalApprox_PredictBounds
        output: dfBounds
        params:
          dfTransformed: dfTransformed
          strType: rate
          vThreshold: vThreshold
      - name: Flag_NormalApprox
        output: dfFlagged
        params:
          dfAnalyzed: dfAnalyzed
          vThreshold: vThreshold
      - name: Summarize
        output: dfSummary
        params:
          dfFlagged: dfFlagged

Now that this new workflow has been saved with a unique file name, we can call it with `MakeWorkflowList()` as above

```{r eval = FALSE, include = TRUE}
library(gsm)
library(clindata)
library(yaml)

## Read in workflow mapping
wf_mapping <- MakeWorkflowList(strNames = "mapping")
wfmetrics_custom <- MakeWorkflowList("kri0004_filtered")
dfMetrics <- wf_metrics %>% map_df(~.x$meta)

# Create Mapped Data
lMapped <- RunWorkflow(lWorkflow = wf_mapping, lData = lRaw)

# Run Metrics
lResults <- RunWorkflow(lWorkflow=wfmetrics_custom, lData=lMapped)
```


# Appendix A: Full `mapping.yaml` file specification

The following YAML file creates all modified rawplus dataframes needed to run all default workflows specified in the `{gsm}` package. Additional `RunQuery()` steps can be added to this file for additional user-defined workflows.

    meta:
      file: mapping.yaml
      description: Data Mappings to prepare for metric calculations
    steps:
      - name: RunQuery
        output: dfEnrolled
       params:
         df: dfSUBJ
         strQuery: "SELECT subjectid as raw_subjectid, * FROM df WHERE enrollyn == 'Y'"
     - name: RunQuery
       output: dfSeriousAE
        params:
         df: dfAE
         strQuery: "SELECT * FROM df WHERE aeser = 'Y'"  
     - name: RunQuery
        output: dfNonimportantPD
        params:
          df: dfPD
          strQuery: "SELECT subjectenrollmentnumber as subjid, * FROM df WHERE deemedimportant == 'No'"
      - name: RunQuery
        output: dfImportantPD
        params:
          df: dfPD
          strQuery: "SELECT subjectenrollmentnumber as subjid, * FROM df WHERE deemedimportant == 'Yes'"
      - name: RunQuery
        output: dfAllLabs
        params:
          df: dfLB
          strQuery: "SELECT * FROM df WHERE toxgrg_nsv IN ('0', '1', '2', '3', '4')"
      - name: RunQuery
        output: dfToxLabs
        params:
          df: dfLB
          strQuery: "SELECT * FROM df WHERE toxgrg_nsv IN ('3', '4')"
      - name: RunQuery
        output: dfStudyDropouts
        params:
          df: dfSTUDCOMP
          strQuery: "SELECT * FROM df WHERE compyn IN ('N')"  
      - name: RunQuery
        output: dfTreatmentDropouts
        params:
          df: dfSDRGCOMP
          strQuery: "SELECT * FROM df WHERE sdrgyn IN ('N') AND phase = 'Blinded Study Drug Completion'"  
      - name: RunQuery
        output: dfValidQueries
        params:
          df: dfQUERY
          strQuery: "SELECT subjectname as subject_nsv, * FROM df WHERE querystatus IN ('Open', 'Answered', 'Closed')"
      - name: RunQuery
        output: dfOldValidQueries
        params:
          df: dfValidQueries
          strQuery: "SELECT * FROM df WHERE queryage > 30"  
      - name: RunQuery
        output: dfDataChanges
        params:
          df: dfDATACHG
          strQuery: "SELECT subjectname as subject_nsv, * FROM df"    
      - name: RunQuery
        output: dfQuery
        params:
          df: dfQUERY
          strQuery: "SELECT subjectname as subject_nsv, * FROM df"  
      - name: RunQuery
        output: dfDataEntry
        params:
          df: dfDATAENT
          strQuery: "SELECT subjectname as subject_nsv, * FROM df"
      - name: RunQuery
        output: dfSlowDataEntry
        params:
          df: dfDATAENT
          strQuery: "SELECT subjectname as subject_nsv, * FROM df WHERE data_entry_lag > 10"  
      - name: RunQuery
        output: dfChangedDataPoints
        params:
          df: dfDATACHG
          strQuery: "SELECT subjectname as subject_nsv, * FROM df WHERE n_changes > 0"  
      - name: RunQuery
        output: dfScreened
        params:
          df: dfENROLL
          strQuery: "SELECT * FROM df"
      - name: RunQuery
        output: dfScreenFail
        params:
          df: dfENROLL
          strQuery: "SELECT * FROM df WHERE enrollyn = 'N'" 

